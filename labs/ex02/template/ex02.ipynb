{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    loss = 1 / 2 * np.mean(e**2)\n",
    "    return loss\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            #print(i,j)\n",
    "            w = np.array([grid_w0[i],grid_w1[j]])\n",
    "            losses[i,j] = compute_loss(y,tx,w)\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.031 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(e) / len(e)\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        \n",
    "        print(w)\n",
    "        print(gamma*gradient)\n",
    "        w = w - gamma*gradient\n",
    "        print(w.shape)\n",
    "        #print(w.shape)\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[-51.3057454  -9.4357987]\n",
      "(2,)\n",
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.305745401473644, w1=9.435798704492269\n",
      "[51.3057454  9.4357987]\n",
      "[-15.39172362  -2.83073961]\n",
      "(2,)\n",
      "GD iter. 1/49: loss=265.3024621089598, w0=66.69746902191571, w1=12.266538315840005\n",
      "[66.69746902 12.26653832]\n",
      "[-4.61751709 -0.84922188]\n",
      "(2,)\n",
      "GD iter. 2/49: loss=37.87837955044126, w0=71.31498610804834, w1=13.115760199244333\n",
      "[71.31498611 13.1157602 ]\n",
      "[-1.38525513 -0.25476657]\n",
      "(2,)\n",
      "GD iter. 3/49: loss=17.410212120174467, w0=72.70024123388814, w1=13.370526764265632\n",
      "[72.70024123 13.37052676]\n",
      "[-0.41557654 -0.07642997]\n",
      "(2,)\n",
      "GD iter. 4/49: loss=15.568077051450455, w0=73.11581777164007, w1=13.446956733772023\n",
      "[73.11581777 13.44695673]\n",
      "[-0.12467296 -0.02292899]\n",
      "(2,)\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "[73.24049073 13.46988572]\n",
      "[-0.03740189 -0.0068787 ]\n",
      "(2,)\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "[73.27789262 13.47676442]\n",
      "[-0.01122057 -0.00206361]\n",
      "(2,)\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "[73.28911319 13.47882803]\n",
      "[-0.00336617 -0.00061908]\n",
      "(2,)\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "[73.29247936 13.47944711]\n",
      "[-0.00100985 -0.00018572]\n",
      "(2,)\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "[73.29348921 13.47963284]\n",
      "[-3.02955296e-04 -5.57174478e-05]\n",
      "(2,)\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "[73.29379216 13.47968856]\n",
      "[-9.08865888e-05 -1.67152343e-05]\n",
      "(2,)\n",
      "GD iter. 11/49: loss=15.385887877543452, w0=73.29388305070998, w1=13.479705271317192\n",
      "[73.29388305 13.47970527]\n",
      "[-2.72659766e-05 -5.01457030e-06]\n",
      "(2,)\n",
      "GD iter. 12/49: loss=15.385887869613665, w0=73.29391031668663, w1=13.479710285887492\n",
      "[73.29391032 13.47971029]\n",
      "[-8.17979299e-06 -1.50437109e-06]\n",
      "(2,)\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "[73.2939185  13.47971179]\n",
      "[-2.45393790e-06 -4.51311326e-07]\n",
      "(2,)\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "[73.29392095 13.47971224]\n",
      "[-7.36181372e-07 -1.35393398e-07]\n",
      "(2,)\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "[73.29392169 13.47971238]\n",
      "[-2.20854408e-07 -4.06180198e-08]\n",
      "(2,)\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "[73.29392191 13.47971242]\n",
      "[-6.62563259e-08 -1.21854064e-08]\n",
      "(2,)\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "[73.29392197 13.47971243]\n",
      "[-1.98769017e-08 -3.65562180e-09]\n",
      "(2,)\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "[73.29392199 13.47971243]\n",
      "[-5.96306740e-09 -1.09668665e-09]\n",
      "(2,)\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "[73.293922   13.47971243]\n",
      "[-1.78891471e-09 -3.29005812e-10]\n",
      "(2,)\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "[73.293922   13.47971243]\n",
      "[-5.36673087e-10 -9.87013874e-11]\n",
      "(2,)\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "[73.293922   13.47971243]\n",
      "[-1.61000971e-10 -2.96105145e-11]\n",
      "(2,)\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "[73.293922   13.47971243]\n",
      "[-4.83040094e-11 -8.88319335e-12]\n",
      "(2,)\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "[73.293922   13.47971243]\n",
      "[-1.44929345e-11 -2.66474672e-12]\n",
      "(2,)\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "[73.293922   13.47971243]\n",
      "[-4.34523827e-12 -7.99619784e-13]\n",
      "(2,)\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "[73.293922   13.47971243]\n",
      "[-1.30105036e-12 -2.40103191e-13]\n",
      "(2,)\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "[73.293922   13.47971243]\n",
      "[-3.87844921e-13 -7.22712912e-14]\n",
      "(2,)\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "[73.293922   13.47971243]\n",
      "[-1.19371180e-13 -2.13594831e-14]\n",
      "(2,)\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "[73.293922   13.47971243]\n",
      "[-3.80077836e-14 -6.29086117e-15]\n",
      "(2,)\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "[73.293922   13.47971243]\n",
      "[-7.76708475e-15 -1.48020263e-15]\n",
      "(2,)\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "[73.293922   13.47971243]\n",
      "[ 1.52795110e-15 -7.16227078e-17]\n",
      "(2,)\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.011 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5513ccde777049908152988916adfc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(e) / len(e)\n",
    "    return grad\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        gradients = np.zeros((batch_size,2))\n",
    "        for y_batch, x_batch in batch_iter(y,tx,batch_size):\n",
    "            for i in range(len(y_batch)):\n",
    "                y_batch_array = np.array([y_batch[i]])\n",
    "                x_batch_array = x_batch[i].reshape(1, -1)\n",
    "                grad = compute_stoch_gradient(y_batch_array,x_batch_array,w)\n",
    "                #print(grad)\n",
    "                gradients[i] = grad\n",
    "        \n",
    "        g = np.sum(gradients, axis=0)\n",
    "        \n",
    "        print(g)\n",
    "        g = g/batch_size\n",
    "        \n",
    "        w = w - gamma*grad\n",
    "        ws.append(w)\n",
    "\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2,)\n",
      "(1,)\n",
      "[-95.15172266 -97.9760532 ]\n"
     ]
    }
   ],
   "source": [
    "y_prueba = np.array([95.15172266])\n",
    "x_prueba = np.array([[1.,1.02968239]])\n",
    "print(x_prueba.shape)\n",
    "w_prueba = np.array([0,0])\n",
    "print(w_prueba.shape)\n",
    "print(y_prueba.shape)\n",
    "print(compute_stoch_gradient(y_prueba,x_prueba,w_prueba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-66.72246342   7.7030422 ]\n",
      "SGD iter. 0/49: loss=2406.6928138373246, w0=6.672246342430175, w1=-0.7703042203044103\n",
      "[-76.51786371 -12.44184036]\n",
      "SGD iter. 1/49: loss=1906.3574641536645, w0=14.324032713146302, w1=0.4738798160216081\n",
      "[-79.20092573 -38.97457719]\n",
      "SGD iter. 2/49: loss=1430.9791621114105, w0=22.244125285904264, w1=4.371337535435531\n",
      "[-49.12452602  -0.45109514]\n",
      "SGD iter. 3/49: loss=1188.1643105140577, w0=27.156577888382262, w1=4.416447049934462\n",
      "[-26.82375725  32.03671908]\n",
      "SGD iter. 4/49: loss=1092.2634333470749, w0=29.838953613345815, w1=1.2127751415417896\n",
      "[-61.3237539  -47.39073875]\n",
      "SGD iter. 5/49: loss=804.5201819015823, w0=35.97132900348672, w1=5.951849016728944\n",
      "[-37.66007239  23.15188396]\n",
      "SGD iter. 6/49: loss=682.5882369822627, w0=39.73733624251128, w1=3.6366606205517025\n",
      "[-50.99594171 -87.85990917]\n",
      "SGD iter. 7/49: loss=494.11570391550976, w0=44.836930413946654, w1=12.422651537650964\n",
      "[-34.30655806 -30.20873521]\n",
      "SGD iter. 8/49: loss=408.47440679756005, w0=48.26758621994266, w1=15.443525058749065\n",
      "[-32.51058347  31.89263323]\n",
      "SGD iter. 9/49: loss=320.90649528688175, w0=51.518644566611314, w1=12.254261735356474\n",
      "[-12.32294272  10.96365888]\n",
      "SGD iter. 10/49: loss=293.1427044270176, w0=52.750938838232756, w1=11.157895847401695\n",
      "[-12.83599342   9.26057755]\n",
      "SGD iter. 11/49: loss=266.91908704846855, w0=54.03453818062512, w1=10.231838092450007\n",
      "[-12.20971066  20.65189184]\n",
      "SGD iter. 12/49: loss=246.9953993795495, w0=55.255509246533165, w1=8.166648908660862\n",
      "[-6.57680163 -0.5290531 ]\n",
      "SGD iter. 13/49: loss=234.6888507158594, w0=55.91318940993915, w1=8.219554218409895\n",
      "[-5.91486565 -0.47580544]\n",
      "SGD iter. 14/49: loss=223.99274369686594, w0=56.50467597448656, w1=8.267134762344371\n",
      "[-24.35236181 -29.26423904]\n",
      "SGD iter. 15/49: loss=180.36990109148869, w0=58.93991215566417, w1=11.193558666503698\n",
      "[-8.94781874  5.62036022]\n",
      "SGD iter. 16/49: loss=167.3028207929243, w0=59.834694029609516, w1=10.631522644997753\n",
      "[-1.79933672 -0.1447428 ]\n",
      "SGD iter. 17/49: loss=164.7522590104714, w0=60.01462770167523, w1=10.64599692472305\n",
      "[-16.28366554 -14.40251592]\n",
      "SGD iter. 18/49: loss=143.67137398783504, w0=61.64299425590995, w1=12.086248516960275\n",
      "[-20.42019354  -5.72770657]\n",
      "SGD iter. 19/49: loss=121.1508073866895, w0=63.68501361007512, w1=12.65901917417152\n",
      "[-15.56948347  -3.40068118]\n",
      "SGD iter. 20/49: loss=106.80751629976245, w0=65.24196195712898, w1=12.999087291906383\n",
      "[-1.8358818   1.11002508]\n",
      "SGD iter. 21/49: loss=104.9921784872857, w0=65.42555013681078, w1=12.88808478395844\n",
      "[-14.11169586 -12.96118338]\n",
      "SGD iter. 22/49: loss=97.03410467517921, w0=66.8367197225945, w1=14.184203122090661\n",
      "[-111.10443855  439.70370348]\n",
      "SGD iter. 23/49: loss=906.6349934294301, w0=77.94716357752881, w1=-29.78616722594744\n",
      "[ 20.79650856 -11.35269473]\n",
      "SGD iter. 24/49: loss=855.0312704068851, w0=75.86751272117814, w1=-28.65089775308466\n",
      "[ 31.16543619 -27.56543453]\n",
      "SGD iter. 25/49: loss=748.6824816027132, w0=72.75096910224097, w1=-25.89435430004454\n",
      "[-49.47341749 -56.19966255]\n",
      "SGD iter. 26/49: loss=562.6566115788452, w0=77.69831085075147, w1=-20.274388045437867\n",
      "[ 9.49023299 -3.26707934]\n",
      "SGD iter. 27/49: loss=549.4858782666145, w0=76.74928755133503, w1=-19.947680111566388\n",
      "[-9.05829966 -2.14102996]\n",
      "SGD iter. 28/49: loss=545.7145666377312, w0=77.6551175173259, w1=-19.733577116010068\n",
      "[-46.7007506  -70.57919843]\n",
      "SGD iter. 29/49: loss=381.11808920684604, w0=82.32519257763832, w1=-12.67565727266988\n",
      "[-37.34819869 -54.03101701]\n",
      "SGD iter. 30/49: loss=305.418610343008, w0=86.06001244703478, w1=-7.272555571824494\n",
      "[-17.31041588 -26.12392094]\n",
      "SGD iter. 31/49: loss=283.26192862712975, w0=87.79105403551138, w1=-4.660163478060573\n",
      "[11.91917576  1.87812212]\n",
      "SGD iter. 32/49: loss=270.5806420123355, w0=86.59913645914212, w1=-4.84797569028348\n",
      "[ 19.60702895 -12.05360559]\n",
      "SGD iter. 33/49: loss=229.51446504019165, w0=84.63843356384882, w1=-3.642615131096158\n",
      "[-10.27649911  -7.94163523]\n",
      "SGD iter. 34/49: loss=229.56440663211308, w0=85.66608347437594, w1=-2.8484516081573616\n",
      "[6.11529825 0.32574719]\n",
      "SGD iter. 35/49: loss=223.11147497526264, w0=85.05455364930963, w1=-2.8810263269443177\n",
      "[ 37.45102935 -50.4789843 ]\n",
      "SGD iter. 36/49: loss=131.4723541036613, w0=81.30945071433132, w1=2.166872102578433\n",
      "[ 25.32682116 -25.45363321]\n",
      "SGD iter. 37/49: loss=97.00590637326236, w0=78.77676859874514, w1=4.7122354240576545\n",
      "[1.70110909 1.20444905]\n",
      "SGD iter. 38/49: loss=96.9881149063162, w0=78.60665768985024, w1=4.5917905188568975\n",
      "[ 20.2899373  -15.85984115]\n",
      "SGD iter. 39/49: loss=80.87623242990841, w0=76.57766395974839, w1=6.177774633972059\n",
      "[4.54621351 0.27489366]\n",
      "SGD iter. 40/49: loss=79.97243462275591, w0=76.12304260845372, w1=6.150285267745303\n",
      "[-8.80636106 -3.49898073]\n",
      "SGD iter. 41/49: loss=80.52220913709616, w0=77.00367871460348, w1=6.500183340346382\n",
      "[ 9.53077557 -1.10031858]\n",
      "SGD iter. 42/49: loss=77.68536378711745, w0=76.05060115731608, w1=6.610215198730752\n",
      "[ 13.56247609 -15.19448232]\n",
      "SGD iter. 43/49: loss=70.34719595404218, w0=74.69435354800255, w1=8.129663430943259\n",
      "[11.58362614 -8.10330139]\n",
      "SGD iter. 44/49: loss=68.2664493766703, w0=73.53599093437916, w1=8.939993570362699\n",
      "[-5.22765919 -3.80264294]\n",
      "SGD iter. 45/49: loss=67.40076106830746, w0=74.0587568538132, w1=9.320257864467493\n",
      "[ 8.83781954 -0.31939584]\n",
      "SGD iter. 46/49: loss=67.74503895963021, w0=73.17497490017385, w1=9.352197448064722\n",
      "[-10.29088395 -10.76080384]\n",
      "SGD iter. 47/49: loss=66.12400526051808, w0=74.20406329533711, w1=10.428277832134869\n",
      "[ 2.34688552 -0.36938006]\n",
      "SGD iter. 48/49: loss=66.09784154205497, w0=73.96937474307161, w1=10.465215837831826\n",
      "[5.72946564 1.45980336]\n",
      "SGD iter. 49/49: loss=66.41218825743992, w0=73.39642817867282, w1=10.319235501834967\n",
      "SGD: execution time=0.005 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43114c2d5630402d98a0951bf9a60dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    loss = 1 / len(y) * np.sum(np.abs(e))\n",
    "    return loss\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.06780585492638"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(y,tx,w_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-59.46138322  29.32101013]\n",
      "SGD iter. 0/49: loss=1090.268711431363, w0=41.622968255137145, w1=-20.52470709254042\n",
      "[-33.72628852  -2.51310695]\n",
      "SGD iter. 1/49: loss=549.0047549992911, w0=65.23137022138106, w1=-18.765532229735303\n",
      "[-15.1672795  -0.8079244]\n",
      "SGD iter. 2/49: loss=494.85521000032867, w0=75.84846586819678, w1=-18.199985152793094\n",
      "[20.88270287 -6.48130495]\n",
      "SGD iter. 3/49: loss=453.3227687215206, w0=61.230573857293784, w1=-13.663071687490394\n",
      "[-0.34237042  0.21047544]\n",
      "SGD iter. 4/49: loss=453.92459925509445, w0=61.47023314862266, w1=-13.810404492528866\n",
      "[-4.77594425  1.74275489]\n",
      "SGD iter. 5/49: loss=448.45084802181395, w0=64.81339412574958, w1=-15.030332917118717\n",
      "[-20.69540325  -3.36508223]\n",
      "SGD iter. 6/49: loss=360.6938118741528, w0=79.30017639909175, w1=-12.67477535591929\n",
      "[ 30.74315055 -21.50630656]\n",
      "SGD iter. 7/49: loss=236.03423705514612, w0=57.77997101248171, w1=2.379639234274281\n",
      "[-6.32493719  1.35947004]\n",
      "SGD iter. 8/49: loss=182.4111392688288, w0=62.20742704768871, w1=1.428010205430378\n",
      "[-9.09906352  8.04800672]\n",
      "SGD iter. 9/49: loss=197.14271718656, w0=68.57677151360896, w1=-4.205594497229717\n",
      "[ 25.84587525 -44.72897835]\n",
      "SGD iter. 10/49: loss=473.13230513638166, w0=50.48465883543212, w1=27.104690344602922\n",
      "[-27.39727222  17.20894704]\n",
      "SGD iter. 11/49: loss=83.72739844621105, w0=69.66274938785577, w1=15.05842741949664\n",
      "[ 0.34651688 -0.40003579]\n",
      "SGD iter. 12/49: loss=85.99121196903153, w0=69.42018757384132, w1=15.3384524694354\n",
      "[-10.89875874  -9.54408191]\n",
      "SGD iter. 13/49: loss=130.7041252816905, w0=77.04931869161987, w1=22.019309805475224\n",
      "[-0.22062683  0.26519928]\n",
      "SGD iter. 14/49: loss=129.15460192068602, w0=77.20375746942541, w1=21.83367031081089\n",
      "[ 0.06585799 -0.07602964]\n",
      "SGD iter. 15/49: loss=129.5872304284379, w0=77.15765687767994, w1=21.886891056039005\n",
      "[ 7.03428642 -2.83931755]\n",
      "SGD iter. 16/49: loss=150.03939959078102, w0=72.23365638538009, w1=23.87441334215161\n",
      "[ 0.56458294 -0.54893527]\n",
      "SGD iter. 17/49: loss=155.8498359172198, w0=71.83844832624939, w1=24.25866802793362\n",
      "[-0.6690334  -0.58587494]\n",
      "SGD iter. 18/49: loss=160.4227692458606, w0=72.30677170672281, w1=24.66878048291341\n",
      "[-5.77823693 -0.30779274]\n",
      "SGD iter. 19/49: loss=164.4405620248498, w0=76.35153756077477, w1=24.884235404086773\n",
      "[-7.89538208  7.85686109]\n",
      "SGD iter. 20/49: loss=131.29006921671944, w0=81.87830501779874, w1=19.384432640920235\n",
      "[18.12804189 10.96853368]\n",
      "SGD iter. 21/49: loss=78.059184891394, w0=69.18867569626859, w1=11.706459064303298\n",
      "[ 1.11471319 -0.12869253]\n",
      "SGD iter. 22/49: loss=82.23535518997059, w0=68.4083764649055, w1=11.796543836383274\n",
      "[-7.12222648  3.42236167]\n",
      "SGD iter. 23/49: loss=67.49276591934023, w0=73.393935003981, w1=9.400890670853768\n",
      "[-4.96950533 -0.18722718]\n",
      "SGD iter. 24/49: loss=70.99355569764795, w0=76.87258873679386, w1=9.531949698768344\n",
      "[ 14.38777456 -17.26589618]\n",
      "SGD iter. 25/49: loss=148.33477117857322, w0=66.80114654244498, w1=21.618077023935143\n",
      "[-8.20024284  0.28075101]\n",
      "SGD iter. 26/49: loss=121.03713105905271, w0=72.54131652789951, w1=21.42155131777241\n",
      "[-7.63204723  8.7204942 ]\n",
      "SGD iter. 27/49: loss=82.38053505998026, w0=77.88374958860554, w1=15.317205379736134\n",
      "[-1.29283351 -1.87032071]\n",
      "SGD iter. 28/49: loss=92.70693875393466, w0=78.7887330461494, w1=16.62642987507502\n",
      "[17.63694646  1.41875667]\n",
      "SGD iter. 29/49: loss=105.57321537795504, w0=66.44287052067355, w1=15.633300209058184\n",
      "[-0.24713012 -0.41247104]\n",
      "SGD iter. 30/49: loss=105.6385112713319, w0=66.61586160258369, w1=15.922029936501685\n",
      "[-4.24194642  2.66447808]\n",
      "SGD iter. 31/49: loss=80.54374295320493, w0=69.58522409855397, w1=14.056895279397493\n",
      "[-3.82917486  2.00167781]\n",
      "SGD iter. 32/49: loss=68.86815764853716, w0=72.26564650218396, w1=12.655720815679759\n",
      "[ 3.95116302 -2.23205033]\n",
      "SGD iter. 33/49: loss=81.43049676709947, w0=69.49983238787702, w1=14.218156044925792\n",
      "[ 2.74822937 -2.10247275]\n",
      "SGD iter. 34/49: loss=97.83651119244192, w0=67.5760718254765, w1=15.689886968083346\n",
      "[-2.78693976  0.79925267]\n",
      "SGD iter. 35/49: loss=84.62713096422723, w0=69.52692965410408, w1=15.130410102446357\n",
      "[-1.74292446 -0.01600473]\n",
      "SGD iter. 36/49: loss=79.87725159863528, w0=70.74697677688796, w1=15.141613413262391\n",
      "[-1.29915071  1.45548072]\n",
      "SGD iter. 37/49: loss=73.60571969906546, w0=71.65638227376496, w1=14.12277690796267\n",
      "[2.44525178 4.08123282]\n",
      "SGD iter. 38/49: loss=74.45739110717045, w0=69.94470602608033, w1=11.265913935556766\n",
      "[-9.6160446   5.24934359]\n",
      "SGD iter. 39/49: loss=75.26082475013901, w0=76.67593724931965, w1=7.5913734235335095\n",
      "[-4.26267266 -6.4329895 ]\n",
      "SGD iter. 40/49: loss=82.12732030284602, w0=79.65980811323182, w1=12.094466073408382\n",
      "[4.95575235 3.17634727]\n",
      "SGD iter. 41/49: loss=68.86154170077494, w0=76.19078146545544, w1=9.87102298582282\n",
      "[-4.85090686 -4.51866754]\n",
      "SGD iter. 42/49: loss=83.15665165280483, w0=79.5864162661515, w1=13.034090261522115\n",
      "[0.60215888 0.52874795]\n",
      "SGD iter. 43/49: loss=80.2478777149862, w0=79.1649050501105, w1=12.663966693508941\n",
      "[1.06544357 0.25182945]\n",
      "SGD iter. 44/49: loss=76.45288790978704, w0=78.41909454948338, w1=12.487686079821188\n",
      "[  5.87435728 -11.36232363]\n",
      "SGD iter. 45/49: loss=110.1996951448454, w0=74.30704445155658, w1=20.441312620818326\n",
      "[ 9.74546976 15.21162738]\n",
      "SGD iter. 46/49: loss=88.36691324412214, w0=67.48521561839723, w1=9.79317345561201\n",
      "[-6.52883723 -2.69078811]\n",
      "SGD iter. 47/49: loss=68.16158843998292, w0=72.0554016759589, w1=11.676725135790555\n",
      "[ 6.83610706 -3.37095361]\n",
      "SGD iter. 48/49: loss=93.5394453243973, w0=67.27012673446228, w1=14.036392661889433\n",
      "[-12.05698828   9.03641861]\n",
      "SGD iter. 49/49: loss=72.80363349164571, w0=75.71001853364335, w1=7.710899634953734\n",
      "GD: execution time=0.005 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "#raise NotImplementedError\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c99c9ba5a75416484483d5b5a0596ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -np.dot(tx.T, np.sign(err)) / len(err)\n",
    "    return grad, err\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        subgrad, err = compute_subgradient_mae(y, tx, w)\n",
    "        w = w -gamma*subgrad\n",
    "        ws.append(w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        \n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=73.36780585492637, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=72.66780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=71.96780585492638, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.26780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=70.56780585492639, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=69.86780585492637, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.16780585492639, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=68.46780585492637, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=67.76780585492638, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.06780585492639, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=66.36780585492637, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=65.66780585492639, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=64.96780585492638, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.26780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=63.567805854926384, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=62.867805854926374, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.167805854926385, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=61.46780585492638, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=60.76780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.067805854926384, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=59.36780585492639, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=58.667805854926385, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=57.96780585492638, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.26780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=56.567805854926384, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=55.867805854926395, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.167805854926385, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=54.46780585492638, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=53.767805854926394, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.06780585492639, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=52.367805854926395, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=51.667805854926385, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=50.96780585492639, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.267805854926394, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=49.56780585492639, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=48.867805854926395, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.1678058549264, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=47.4678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=46.7678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.067805854926405, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=45.367805854926395, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=44.6678058549264, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=43.9678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.2678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=42.567805854926405, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=41.867805854926395, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.167805854926385, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=40.46780585492639, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=39.767805854926394, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.067805854926384, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=38.36780585492638, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=37.66780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=36.96780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.26780585492637, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=35.56780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=34.86780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.16780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=33.46780585492636, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=32.767805854926365, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.067805854926355, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=31.36780585492636, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=30.667805854926353, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=29.96780585492635, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.267805854926348, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=28.567805854926345, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=27.867805854926342, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.17327020966892, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=26.490451563751197, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=25.817212322770175, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.15503943465645, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=24.524103413894778, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=23.899295346035593, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.284392925657148, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=22.686876444181845, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.10626756964055, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=21.537818828008433, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=20.986339874628463, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.445560936620446, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=19.91191015895785, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.389644090563234, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=18.887989064395885, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.415960501854236, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=17.954898543040386, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.505757656579824, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.07495742693161, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=16.652967297509903, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.24854073149673, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=15.849105212654159, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.46691979123133, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.108294621512217, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=14.754896345922832, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.40452896162028, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.055787028127279, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=13.714620911605637, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.381236307284155, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.058821615166238, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=12.74025172433924, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.423218888756113, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.107561731901173, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=11.800622097398135, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.49504179464643, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.189461491894715, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=10.883881189143004, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.584593408313204, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.295816534318941, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.01135208122136, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=9.728084326668132, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.44812546112251, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.17104110409667, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=8.903656131158964, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.636271158221257, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.376151920302375, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.140540838751496, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=7.918544501597273, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.7052797283770005, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.493695831178641, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.289992405743416, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.097234035781543, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=6.919905294668923, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.750573527315454, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.584744810805664, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.430343276347806, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.278071481890353, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.133663329263324, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.00584079834303, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=5.885021825223219, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.771635252269658, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.667162061790257, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.586726765993147, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.523847812160388, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.480093708591872, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.4530880035020255, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.427392630862905, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.407322445682752, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.387252260502599, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.3704607803386955, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.357406523334741, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.345929264022584, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.335714659517473, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.330043910465361, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.325676428273225, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.322176726526591, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.320111309643114, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.318478284898438, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.3172400485651465, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.316406547951545, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.315557122666144, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.31470769738074, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.313876880922167, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.3130522468713846, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.312377839024388, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312132229725043, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.311886620425697, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311683566098433, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311594306869985, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311549677255759, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311505047641534, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311393473605972, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311214955149073, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311125695920624, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.311081066306399, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310913706061381, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.310862636053835, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310823570190169, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310804671438473, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.310772500182623, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.310749731161021, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310733434318958, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310717105690679, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.310682364311412, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.3106844802203375, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.310662165413225, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310643298447746, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3105922284402, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.310633183099026, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.3105993435850625, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.31061822827579, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.310606458729927, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.3106032734525535, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.310613573874789, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.310588318629318, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310620689019651, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.3105749661498844, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.31058363964973, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=5.310622915165495, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310578960670142, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.310625141311338, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310584557534202, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.31062247845816, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.3105798785546146, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.310624704604003, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310575199575027, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310626930749847, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.31057969173614, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310580796439088, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310624267896667, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310577675701808, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310576117459501, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.31062649404251, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310581714323563, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310623831189332, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.310577035343975, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310626057335176, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.310582632208036, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310623394481999, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105779532284485, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3106256206278415, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.310578699848581, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.310574998409099, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.3105835500925105, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3106229577746635, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.310576683814244, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310578871112923, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310625183920505, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310584467976985, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310622521067328, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310579788997397, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310624747213171, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310575110017809, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310626973359014, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310576022555871, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.31058070688187, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.310624310505837, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.31057770796102, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.310576027902282, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.31062653665168, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.3105816247663435, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.310623873798502, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310576945786757, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310626099944344, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310582542650818, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310623437091167, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.31057786367123, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.310625663237008, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310575030668312, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310583460535291, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.310623000383832, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.310578781555703, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310625226529674, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.310584378419764, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310622563676499, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.310579699440178, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.31062478982234, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.3105780708494175, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.310575020460591, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.3106270159681825, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.310579756254566, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310580617324651, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.3106243531150055, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310575938345063, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310626579260848, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310581535209124, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310623916407671, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310576856229536, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310626142553512, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310579094996193, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310575393556711, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.310582453093599, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310623479700335, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.3105777741140106, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.310625705846178, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.3105833709780725, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.310623042993, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.31057674833267, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.310578691998486, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310625269138844, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.310584288862547, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.3106226062856665, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310579609882959, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310624832431509, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310578103108631, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.310574930903372, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310627058577351, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310580527767432, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310624395724174, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.3105758487878445, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.310626621870017, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.31057945788459, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.310581445651906, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310623959016839, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310576766672319, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310626185162682, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310582363536381, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.310623522309504, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.310577684556793, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310625748455347, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310583281420854, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.3106230856021694, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310578602441266, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310625311748012, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.310584199305326, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310622648894835, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.31057952032574, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.310624875040678, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.310574841346153, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.31062710118652, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.310580438210215, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310624438333343, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.310575759230627, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310626664479185, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.310579490143804, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310581356094687, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310624001626008, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.310576677115099, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.31062622777185, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310575458075135, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310582273979162, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310623564918673, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.3105775949995735, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.310625791064514, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310583191863635, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.310623128211338, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310576812851097, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310578512884048, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.31062535435718, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.31058410974811, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.310622691504004, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310579430768521, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310624917649847, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310574751788934, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310627143795689, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310580348652995, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310624480942511, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.310575669673407, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.310626707088354, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.3105812665374685, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.310624044235177, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.31057658755788, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.31062627038102, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310582184421943, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310623607527842, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577505442355, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310625833673684, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310575159705162, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310583102306416, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310623170820506, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310578423326827, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.3106253969663495, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.3105840201908885, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.3106227341131715, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.310576514481122, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.3105793412113025, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.310624960259014, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.31057819988627, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.310574662231715, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310627186404858, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310579885291417, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310580259095775, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310624523551678, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310575580116187, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310626749697524, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.310581176980249, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310624086844346, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105764980006605, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.310626312990188, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.310582094864723, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.31062365013701, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.310577207998708, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.3105774158851355, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.310625876282853, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.3105830127491975, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.310623213429675, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310578333769609, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.310625439575518, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310583930633672, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310622776722341, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310579251654084, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.009 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0deb05e55c84218aacd65f425e59644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=1001, min=1), Output()), _dom_classes=('wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for y_batch, tx_batch in batch_iter(\n",
    "            y, tx, batch_size=batch_size, num_batches=1\n",
    "        ):\n",
    "            # compute a stochastic subgradient and loss\n",
    "            grad, err = compute_subgradient_mae(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic subgradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=73.36780585492637, w0=0.7, w1=-0.4110751845422259\n",
      "SubSGD iter. 1/499: loss=72.66780585492637, w0=1.4, w1=0.11698063096842282\n",
      "SubSGD iter. 2/499: loss=71.96780585492638, w0=2.0999999999999996, w1=0.7361140269130894\n",
      "SubSGD iter. 3/499: loss=71.26780585492638, w0=2.8, w1=0.7014231675716183\n",
      "SubSGD iter. 4/499: loss=70.56780585492638, w0=3.5, w1=0.581531354083287\n",
      "SubSGD iter. 5/499: loss=69.86780585492637, w0=4.2, w1=1.460779319979459\n",
      "SubSGD iter. 6/499: loss=69.16780585492639, w0=4.9, w1=1.900703762863641\n",
      "SubSGD iter. 7/499: loss=68.46780585492637, w0=5.6000000000000005, w1=1.9379910744815108\n",
      "SubSGD iter. 8/499: loss=67.76780585492638, w0=6.300000000000001, w1=1.2596440785069678\n",
      "SubSGD iter. 9/499: loss=67.06780585492639, w0=7.000000000000001, w1=1.7688299224532202\n",
      "SubSGD iter. 10/499: loss=66.36780585492637, w0=7.700000000000001, w1=2.293010473904938\n",
      "SubSGD iter. 11/499: loss=65.66780585492637, w0=8.4, w1=2.8021963178511906\n",
      "SubSGD iter. 12/499: loss=64.96780585492638, w0=9.1, w1=3.067680020075421\n",
      "SubSGD iter. 13/499: loss=64.26780585492638, w0=9.799999999999999, w1=3.1815003468897336\n",
      "SubSGD iter. 14/499: loss=63.56780585492637, w0=10.499999999999998, w1=4.086675450076797\n",
      "SubSGD iter. 15/499: loss=62.867805854926374, w0=11.199999999999998, w1=4.375172714634212\n",
      "SubSGD iter. 16/499: loss=62.16780585492637, w0=11.899999999999997, w1=4.640656416858443\n",
      "SubSGD iter. 17/499: loss=61.46780585492638, w0=12.599999999999996, w1=5.587568678600873\n",
      "SubSGD iter. 18/499: loss=60.76780585492637, w0=13.299999999999995, w1=6.083194939069706\n",
      "SubSGD iter. 19/499: loss=60.067805854926384, w0=13.999999999999995, w1=6.197015265884019\n",
      "SubSGD iter. 20/499: loss=59.367805854926374, w0=14.699999999999994, w1=6.928979859391654\n",
      "SubSGD iter. 21/499: loss=58.66780585492638, w0=15.399999999999993, w1=6.657499677409074\n",
      "SubSGD iter. 22/499: loss=57.96780585492638, w0=16.099999999999994, w1=6.217304422702103\n",
      "SubSGD iter. 23/499: loss=57.26780585492638, w0=16.799999999999994, w1=5.8809414650260265\n",
      "SubSGD iter. 24/499: loss=56.56780585492638, w0=17.499999999999993, w1=6.72213163325067\n",
      "SubSGD iter. 25/499: loss=55.867805854926374, w0=18.199999999999992, w1=7.680266724152477\n",
      "SubSGD iter. 26/499: loss=55.16780585492638, w0=18.89999999999999, w1=6.32630983011925\n",
      "SubSGD iter. 27/499: loss=54.46780585492638, w0=19.59999999999999, w1=6.528746720160249\n",
      "SubSGD iter. 28/499: loss=53.76780585492638, w0=20.29999999999999, w1=5.8718657524072855\n",
      "SubSGD iter. 29/499: loss=53.067805854926384, w0=20.99999999999999, w1=5.259067581405873\n",
      "SubSGD iter. 30/499: loss=52.367805854926395, w0=21.69999999999999, w1=4.464698236915092\n",
      "SubSGD iter. 31/499: loss=51.667805854926385, w0=22.399999999999988, w1=5.013092740672903\n",
      "SubSGD iter. 32/499: loss=50.96780585492638, w0=23.099999999999987, w1=4.20497899963097\n",
      "SubSGD iter. 33/499: loss=50.267805854926394, w0=23.799999999999986, w1=3.396865258589037\n",
      "SubSGD iter. 34/499: loss=49.56780585492639, w0=24.499999999999986, w1=2.891847526764715\n",
      "SubSGD iter. 35/499: loss=48.867805854926395, w0=25.199999999999985, w1=3.949762833592273\n",
      "SubSGD iter. 36/499: loss=48.167805854926385, w0=25.899999999999984, w1=4.3733040866462884\n",
      "SubSGD iter. 37/499: loss=47.4678058549264, w0=26.599999999999984, w1=4.093375094209826\n",
      "SubSGD iter. 38/499: loss=46.767805854926394, w0=27.299999999999983, w1=5.040287355952256\n",
      "SubSGD iter. 39/499: loss=46.06780585492639, w0=27.999999999999982, w1=5.506869481918831\n",
      "SubSGD iter. 40/499: loss=45.367805854926395, w0=28.69999999999998, w1=5.136601391390035\n",
      "SubSGD iter. 41/499: loss=44.6678058549264, w0=29.39999999999998, w1=4.300562534117219\n",
      "SubSGD iter. 42/499: loss=43.9678058549264, w0=30.09999999999998, w1=4.015388039567925\n",
      "SubSGD iter. 43/499: loss=43.267805854926394, w0=30.79999999999998, w1=4.363355085958339\n",
      "SubSGD iter. 44/499: loss=42.567805854926405, w0=31.49999999999998, w1=4.026992128282263\n",
      "SubSGD iter. 45/499: loss=41.867805854926395, w0=32.19999999999998, w1=4.658157468277116\n",
      "SubSGD iter. 46/499: loss=41.167805854926385, w0=32.899999999999984, w1=5.108337214483409\n",
      "SubSGD iter. 47/499: loss=40.46780585492638, w0=33.59999999999999, w1=5.4520498356668\n",
      "SubSGD iter. 48/499: loss=39.76780585492639, w0=34.29999999999999, w1=4.848283765700287\n",
      "SubSGD iter. 49/499: loss=39.06780585492638, w0=34.99999999999999, w1=4.278323641164301\n",
      "SubSGD iter. 50/499: loss=38.367805854926374, w0=35.699999999999996, w1=4.077574104115049\n",
      "SubSGD iter. 51/499: loss=37.66780585492637, w0=36.4, w1=3.6664989195728235\n",
      "SubSGD iter. 52/499: loss=36.96780585492637, w0=37.1, w1=3.1614811877485014\n",
      "SubSGD iter. 53/499: loss=36.267805854926365, w0=37.800000000000004, w1=3.3272874473159986\n",
      "SubSGD iter. 54/499: loss=35.56780585492637, w0=38.50000000000001, w1=2.92782967492942\n",
      "SubSGD iter. 55/499: loss=34.86780585492637, w0=39.20000000000001, w1=4.467892013316606\n",
      "SubSGD iter. 56/499: loss=34.167805854926364, w0=39.90000000000001, w1=5.2292489639356745\n",
      "SubSGD iter. 57/499: loss=33.467805854926354, w0=40.600000000000016, w1=4.252451484993756\n",
      "SubSGD iter. 58/499: loss=32.76780585492636, w0=41.30000000000002, w1=5.265129826346523\n",
      "SubSGD iter. 59/499: loss=32.06780585492635, w0=42.00000000000002, w1=4.238197628753989\n",
      "SubSGD iter. 60/499: loss=31.36780585492635, w0=42.700000000000024, w1=4.033364108878766\n",
      "SubSGD iter. 61/499: loss=30.667805854926353, w0=43.40000000000003, w1=3.4792570348821856\n",
      "SubSGD iter. 62/499: loss=29.967805854926343, w0=44.10000000000003, w1=4.027651538639996\n",
      "SubSGD iter. 63/499: loss=29.26780585492634, w0=44.80000000000003, w1=3.1916126813671806\n",
      "SubSGD iter. 64/499: loss=28.567805854926338, w0=45.500000000000036, w1=2.664655645598333\n",
      "SubSGD iter. 65/499: loss=27.867805854926342, w0=46.20000000000004, w1=1.9611507272074702\n",
      "SubSGD iter. 66/499: loss=27.167805854926332, w0=46.90000000000004, w1=1.8803364217187593\n",
      "SubSGD iter. 67/499: loss=26.467805854926333, w0=47.600000000000044, w1=2.759584387614931\n",
      "SubSGD iter. 68/499: loss=25.76780585492633, w0=48.30000000000005, w1=1.9887613215467437\n",
      "SubSGD iter. 69/499: loss=25.067805854926327, w0=49.00000000000005, w1=2.593293427400903\n",
      "SubSGD iter. 70/499: loss=24.37078695963414, w0=49.70000000000005, w1=1.8967086761601406\n",
      "SubSGD iter. 71/499: loss=23.670972549092863, w0=50.400000000000055, w1=2.3202499292141563\n",
      "SubSGD iter. 72/499: loss=22.99060047873738, w0=51.10000000000006, w1=2.1697932527630286\n",
      "SubSGD iter. 73/499: loss=22.366146201111686, w0=51.80000000000006, w1=1.4304604533190846\n",
      "SubSGD iter. 74/499: loss=21.62544406400851, w0=52.500000000000064, w1=2.5230849676448788\n",
      "SubSGD iter. 75/499: loss=20.9762912246611, w0=53.20000000000007, w1=2.2676530865433078\n",
      "SubSGD iter. 76/499: loss=20.254181019303623, w0=53.90000000000007, w1=3.029010037162377\n",
      "SubSGD iter. 77/499: loss=19.549218754674506, w0=54.60000000000007, w1=3.6481434331070437\n",
      "SubSGD iter. 78/499: loss=18.865290256876932, w0=55.300000000000075, w1=3.897741895794222\n",
      "SubSGD iter. 79/499: loss=18.135718497758994, w0=56.00000000000008, w1=4.954143127173111\n",
      "SubSGD iter. 80/499: loss=17.440871963001666, w0=56.70000000000008, w1=5.381660212390529\n",
      "SubSGD iter. 81/499: loss=16.77977742314045, w0=57.400000000000084, w1=5.300845906901818\n",
      "SubSGD iter. 82/499: loss=16.128769276780805, w0=58.10000000000009, w1=5.353006252431529\n",
      "SubSGD iter. 83/499: loss=15.577845263477313, w0=58.80000000000009, w1=5.067831757882234\n",
      "SubSGD iter. 84/499: loss=14.876436839956304, w0=59.50000000000009, w1=5.698997097877086\n",
      "SubSGD iter. 85/499: loss=14.28399448876911, w0=60.200000000000095, w1=5.943309821586221\n",
      "SubSGD iter. 86/499: loss=13.883087840658687, w0=60.9000000000001, w1=5.598132733861222\n",
      "SubSGD iter. 87/499: loss=13.638158066088662, w0=61.6000000000001, w1=4.9789915784468395\n",
      "SubSGD iter. 88/499: loss=13.576141501920981, w0=62.300000000000104, w1=4.184622233956059\n",
      "SubSGD iter. 89/499: loss=13.781674578105559, w0=61.6000000000001, w1=4.616414812239479\n",
      "SubSGD iter. 90/499: loss=13.579563520819637, w0=62.300000000000104, w1=4.176726407154295\n",
      "SubSGD iter. 91/499: loss=13.102062753917945, w0=63.00000000000011, w1=4.454853375955983\n",
      "SubSGD iter. 92/499: loss=12.991809144894122, w0=62.300000000000104, w1=5.5807857894571535\n",
      "SubSGD iter. 93/499: loss=13.045904256046814, w0=61.6000000000001, w1=6.764787989851045\n",
      "SubSGD iter. 94/499: loss=12.741240534918543, w0=62.300000000000104, w1=6.259770258026722\n",
      "SubSGD iter. 95/499: loss=11.905232691908616, w0=63.00000000000011, w1=7.428101058544191\n",
      "SubSGD iter. 96/499: loss=11.464436307154946, w0=63.70000000000011, w1=7.480261404073901\n",
      "SubSGD iter. 97/499: loss=11.707632639116037, w0=63.00000000000011, w1=8.093059575075314\n",
      "SubSGD iter. 98/499: loss=11.17421713599465, w0=63.70000000000011, w1=8.353771796457178\n",
      "SubSGD iter. 99/499: loss=10.595131568595779, w0=64.4000000000001, w1=8.803951542663471\n",
      "SubSGD iter. 100/499: loss=10.191571094740773, w0=65.10000000000011, w1=8.77152253489611\n",
      "SubSGD iter. 101/499: loss=10.517743954314087, w0=64.4000000000001, w1=9.051451527332574\n",
      "SubSGD iter. 102/499: loss=9.949013193514324, w0=65.10000000000011, w1=9.491375970216756\n",
      "SubSGD iter. 103/499: loss=9.239560751742623, w0=65.80000000000011, w1=10.438288231959186\n",
      "SubSGD iter. 104/499: loss=8.883936669162669, w0=66.50000000000011, w1=10.317783680480144\n",
      "SubSGD iter. 105/499: loss=8.510474559932135, w0=67.20000000000012, w1=10.360110237083635\n",
      "SubSGD iter. 106/499: loss=8.744447773861246, w0=66.50000000000011, w1=10.745394377411763\n",
      "SubSGD iter. 107/499: loss=8.316788053592754, w0=67.20000000000012, w1=10.91120063697926\n",
      "SubSGD iter. 108/499: loss=8.530859269366484, w0=66.50000000000011, w1=11.463172068916938\n",
      "SubSGD iter. 109/499: loss=7.99118996735456, w0=67.20000000000012, w1=12.00412912088808\n",
      "SubSGD iter. 110/499: loss=7.872158390783857, w0=67.90000000000012, w1=11.233306054819893\n",
      "SubSGD iter. 111/499: loss=7.7268259630864975, w0=68.60000000000012, w1=10.883368123339576\n",
      "SubSGD iter. 112/499: loss=7.224103438689063, w0=69.30000000000013, w1=11.514533463334429\n",
      "SubSGD iter. 113/499: loss=7.547231394211098, w0=68.60000000000012, w1=11.336181341954614\n",
      "SubSGD iter. 114/499: loss=7.810209926596002, w0=67.90000000000012, w1=11.416995647443326\n",
      "SubSGD iter. 115/499: loss=7.688886976320324, w0=68.60000000000012, w1=10.97730724235814\n",
      "SubSGD iter. 116/499: loss=7.935627051790114, w0=67.90000000000012, w1=11.058121547846852\n",
      "SubSGD iter. 117/499: loss=7.543251388245329, w0=68.60000000000012, w1=11.346618812404268\n",
      "SubSGD iter. 118/499: loss=7.034562732692059, w0=69.30000000000013, w1=12.003801515599253\n",
      "SubSGD iter. 119/499: loss=6.533632641002265, w0=70.00000000000013, w1=12.765158466218322\n",
      "SubSGD iter. 120/499: loss=6.0149253081036615, w0=70.70000000000013, w1=13.670333569405386\n",
      "SubSGD iter. 121/499: loss=6.032919977005656, w0=70.00000000000013, w1=14.510361140865196\n",
      "SubSGD iter. 122/499: loss=5.738012997199569, w0=70.70000000000013, w1=14.739482833151955\n",
      "SubSGD iter. 123/499: loss=6.0584608063579966, w0=70.00000000000013, w1=14.404254777848701\n",
      "SubSGD iter. 124/499: loss=5.999921835372448, w0=70.70000000000013, w1=13.717560142342565\n",
      "SubSGD iter. 125/499: loss=6.07808549308185, w0=71.40000000000013, w1=12.917729408279856\n",
      "SubSGD iter. 126/499: loss=5.719471227770503, w0=72.10000000000014, w1=13.534116357117105\n",
      "SubSGD iter. 127/499: loss=5.556779486401181, w0=72.80000000000014, w1=13.82261362167452\n",
      "SubSGD iter. 128/499: loss=5.462068852781464, w0=72.10000000000014, w1=14.6169829661653\n",
      "SubSGD iter. 129/499: loss=5.859725985530315, w0=71.40000000000013, w1=13.56058173478641\n",
      "SubSGD iter. 130/499: loss=5.8236803110417865, w0=70.70000000000013, w1=14.344814462825735\n",
      "SubSGD iter. 131/499: loss=5.507853255295205, w0=71.40000000000013, w1=14.961201411662984\n",
      "SubSGD iter. 132/499: loss=5.631165489367045, w0=70.70000000000013, w1=15.508363713896854\n",
      "SubSGD iter. 133/499: loss=5.870353807317952, w0=70.00000000000013, w1=15.522118909872344\n",
      "SubSGD iter. 134/499: loss=5.635957716413969, w0=70.70000000000013, w1=16.72813519964279\n",
      "SubSGD iter. 135/499: loss=5.545272134047983, w0=71.40000000000013, w1=17.380192097085246\n",
      "SubSGD iter. 136/499: loss=5.403214300259049, w0=72.10000000000014, w1=17.004606258798677\n",
      "SubSGD iter. 137/499: loss=5.380494780961503, w0=72.80000000000014, w1=16.894432141917935\n",
      "SubSGD iter. 138/499: loss=5.564955145479204, w0=73.50000000000014, w1=17.62639673542557\n",
      "SubSGD iter. 139/499: loss=5.608759224847738, w0=74.20000000000014, w1=17.26047600469517\n",
      "SubSGD iter. 140/499: loss=5.40239043039372, w0=73.50000000000014, w1=16.608419107252715\n",
      "SubSGD iter. 141/499: loss=5.393035192738437, w0=72.80000000000014, w1=16.97868719778151\n",
      "SubSGD iter. 142/499: loss=5.47840853010105, w0=72.10000000000014, w1=17.456735400207634\n",
      "SubSGD iter. 143/499: loss=5.464797764176946, w0=72.80000000000014, w1=17.438155941308757\n",
      "SubSGD iter. 144/499: loss=5.560412796803828, w0=73.50000000000014, w1=17.603608733241384\n",
      "SubSGD iter. 145/499: loss=5.99801830437613, w0=74.20000000000014, w1=18.842925034219384\n",
      "SubSGD iter. 146/499: loss=5.875290193099309, w0=74.90000000000015, w1=17.716992620718212\n",
      "SubSGD iter. 147/499: loss=5.57974880951589, w0=74.20000000000014, w1=17.08582728072336\n",
      "SubSGD iter. 148/499: loss=5.362006838758791, w0=73.50000000000014, w1=16.127692189821556\n",
      "SubSGD iter. 149/499: loss=5.321948452666909, w0=72.80000000000014, w1=16.41286668437085\n",
      "SubSGD iter. 150/499: loss=5.418694724568264, w0=72.10000000000014, w1=17.116371602761713\n",
      "SubSGD iter. 151/499: loss=5.364190892393896, w0=72.80000000000014, w1=16.780008645085637\n",
      "SubSGD iter. 152/499: loss=5.364084090861263, w0=73.50000000000014, w1=15.654076231584465\n",
      "SubSGD iter. 153/499: loss=5.476191293825956, w0=74.20000000000014, w1=16.00204327797488\n",
      "SubSGD iter. 154/499: loss=5.660884241389896, w0=74.90000000000015, w1=14.81804107758099\n",
      "SubSGD iter. 155/499: loss=5.4655044570216695, w0=74.20000000000014, w1=15.6580686490408\n",
      "SubSGD iter. 156/499: loss=5.640251331476586, w0=74.90000000000015, w1=15.054302579074285\n",
      "SubSGD iter. 157/499: loss=5.467079861204841, w0=74.20000000000014, w1=15.532350781500408\n",
      "SubSGD iter. 158/499: loss=5.39634522670667, w0=73.50000000000014, w1=14.913217385555742\n",
      "SubSGD iter. 159/499: loss=5.465706613799984, w0=72.80000000000014, w1=14.256034682360756\n",
      "SubSGD iter. 160/499: loss=5.829539899484284, w0=73.50000000000014, w1=12.902077788327528\n",
      "SubSGD iter. 161/499: loss=5.69060366797426, w0=72.80000000000014, w1=13.3422730430345\n",
      "SubSGD iter. 162/499: loss=5.611035895637056, w0=73.50000000000014, w1=13.571394735321258\n",
      "SubSGD iter. 163/499: loss=5.497759770011663, w0=72.80000000000014, w1=14.07989585306109\n",
      "SubSGD iter. 164/499: loss=5.405398163197737, w0=73.50000000000014, w1=14.798059177092279\n",
      "SubSGD iter. 165/499: loss=5.3223803626064194, w0=72.80000000000014, w1=15.606172918134211\n",
      "SubSGD iter. 166/499: loss=5.3538474481311376, w0=72.10000000000014, w1=16.507805182546967\n",
      "SubSGD iter. 167/499: loss=5.33931037249895, w0=72.80000000000014, w1=15.296382294620788\n",
      "SubSGD iter. 168/499: loss=5.377300293340631, w0=73.50000000000014, w1=15.263953286853427\n",
      "SubSGD iter. 169/499: loss=5.379021291248903, w0=72.80000000000014, w1=14.840412033799412\n",
      "SubSGD iter. 170/499: loss=5.388237597532578, w0=73.50000000000014, w1=15.03675662731664\n",
      "SubSGD iter. 171/499: loss=5.465580304339192, w0=74.20000000000014, w1=15.651417608447026\n",
      "SubSGD iter. 172/499: loss=5.627531202197833, w0=74.90000000000015, w1=15.794748199041543\n",
      "SubSGD iter. 173/499: loss=5.885712240735092, w0=75.60000000000015, w1=16.07287516784323\n",
      "SubSGD iter. 174/499: loss=5.645708541118525, w0=74.90000000000015, w1=14.988999771151928\n",
      "SubSGD iter. 175/499: loss=5.6159585328618356, w0=74.20000000000014, w1=13.896375256826133\n",
      "SubSGD iter. 176/499: loss=5.412270505659834, w0=73.50000000000014, w1=14.715033626631762\n",
      "SubSGD iter. 177/499: loss=5.379595914101331, w0=72.80000000000014, w1=14.835538178110804\n",
      "SubSGD iter. 178/499: loss=5.435224479771937, w0=72.10000000000014, w1=14.793211621507313\n",
      "SubSGD iter. 179/499: loss=5.517217197741241, w0=72.80000000000014, w1=13.993380887444603\n",
      "SubSGD iter. 180/499: loss=5.464459967156776, w0=73.50000000000014, w1=14.263519337734273\n",
      "SubSGD iter. 181/499: loss=5.396299503047802, w0=72.80000000000014, w1=14.703714592441244\n",
      "SubSGD iter. 182/499: loss=5.348080155278575, w0=72.10000000000014, w1=15.605346856854002\n",
      "SubSGD iter. 183/499: loss=5.43432856826903, w0=71.40000000000013, w1=16.152509159087874\n",
      "SubSGD iter. 184/499: loss=5.612140358110595, w0=70.70000000000013, w1=16.369766163267624\n",
      "SubSGD iter. 185/499: loss=5.450580119218152, w0=71.40000000000013, w1=16.59888785555438\n",
      "SubSGD iter. 186/499: loss=5.645234388317837, w0=70.70000000000013, w1=16.81614485973413\n",
      "SubSGD iter. 187/499: loss=5.86496093428269, w0=70.00000000000013, w1=16.18497951973928\n",
      "SubSGD iter. 188/499: loss=5.6109696279353924, w0=70.70000000000013, w1=16.166400060840402\n",
      "SubSGD iter. 189/499: loss=5.449423694393784, w0=71.40000000000013, w1=15.562633990873888\n",
      "SubSGD iter. 190/499: loss=5.455363609423027, w0=72.10000000000014, w1=14.66100172646113\n",
      "SubSGD iter. 191/499: loss=5.464100914941368, w0=72.80000000000014, w1=14.265025483239297\n",
      "SubSGD iter. 192/499: loss=5.53606407842361, w0=73.50000000000014, w1=13.865567710852718\n",
      "SubSGD iter. 193/499: loss=5.52543313153875, w0=74.20000000000014, w1=14.528712027895764\n",
      "SubSGD iter. 194/499: loss=5.389847252745685, w0=73.50000000000014, w1=15.006760230321888\n",
      "SubSGD iter. 195/499: loss=5.3403018235751665, w0=72.80000000000014, w1=15.278240412304466\n",
      "SubSGD iter. 196/499: loss=5.4062657991586995, w0=73.50000000000014, w1=16.643609864968077\n",
      "SubSGD iter. 197/499: loss=5.466530642310272, w0=74.20000000000014, w1=15.789763594542809\n",
      "SubSGD iter. 198/499: loss=5.3774799230108465, w0=73.50000000000014, w1=15.260062620480692\n",
      "SubSGD iter. 199/499: loss=5.524707865039311, w0=74.20000000000014, w1=16.625432073144303\n",
      "SubSGD iter. 200/499: loss=5.825033210787516, w0=74.90000000000015, w1=17.48165412564884\n",
      "SubSGD iter. 201/499: loss=5.548699785169972, w0=74.20000000000014, w1=16.862520729704176\n",
      "SubSGD iter. 202/499: loss=5.681430449369384, w0=74.90000000000015, w1=16.50940497813686\n",
      "SubSGD iter. 203/499: loss=5.466009887875932, w0=74.20000000000014, w1=15.77512923464506\n",
      "SubSGD iter. 204/499: loss=5.669881925217233, w0=74.90000000000015, w1=16.39151618348231\n",
      "SubSGD iter. 205/499: loss=5.898574726064365, w0=75.60000000000015, w1=16.20805040443189\n",
      "SubSGD iter. 206/499: loss=5.624167318292339, w0=74.90000000000015, w1=15.678349430369774\n",
      "SubSGD iter. 207/499: loss=5.472409492241299, w0=74.20000000000014, w1=15.32332195283995\n",
      "SubSGD iter. 208/499: loss=5.675258437397187, w0=74.90000000000015, w1=16.448451912970256\n",
      "SubSGD iter. 209/499: loss=5.473634947257925, w0=74.20000000000014, w1=15.280121112452788\n",
      "SubSGD iter. 210/499: loss=5.395425275354515, w0=73.50000000000014, w1=14.925093634922963\n",
      "SubSGD iter. 211/499: loss=5.498636598720024, w0=74.20000000000014, w1=14.798079189694937\n",
      "SubSGD iter. 212/499: loss=5.707410685550875, w0=74.90000000000015, w1=14.3677476012342\n",
      "SubSGD iter. 213/499: loss=5.530215921513067, w0=74.20000000000014, w1=14.488252152713242\n",
      "SubSGD iter. 214/499: loss=5.387721414127515, w0=73.50000000000014, w1=15.046375856462282\n",
      "SubSGD iter. 215/499: loss=5.333827326378439, w0=72.80000000000014, w1=15.396712789830673\n",
      "SubSGD iter. 216/499: loss=5.365657462954059, w0=72.10000000000014, w1=15.344552444300962\n",
      "SubSGD iter. 217/499: loss=5.312846622539619, w0=72.80000000000014, w1=15.897758360377075\n",
      "SubSGD iter. 218/499: loss=5.365901895830974, w0=73.50000000000014, w1=15.544642608809761\n",
      "SubSGD iter. 219/499: loss=5.498067949335465, w0=74.20000000000014, w1=14.805309809365816\n",
      "SubSGD iter. 220/499: loss=5.36418183781986, w0=73.50000000000014, w1=15.645337380825627\n",
      "SubSGD iter. 221/499: loss=5.54456562330221, w0=74.20000000000014, w1=16.82614208152552\n",
      "SubSGD iter. 222/499: loss=5.755198266925537, w0=74.90000000000015, w1=17.070454805234657\n",
      "SubSGD iter. 223/499: loss=5.905085508345729, w0=75.60000000000015, w1=16.270624071171948\n",
      "SubSGD iter. 224/499: loss=5.771497637208438, w0=74.90000000000015, w1=17.172256335584706\n",
      "SubSGD iter. 225/499: loss=5.5034825245807, w0=74.20000000000014, w1=16.410899384965635\n",
      "SubSGD iter. 226/499: loss=5.693273552724435, w0=74.90000000000015, w1=16.62206377953184\n",
      "SubSGD iter. 227/499: loss=5.6160562326924675, w0=74.20000000000014, w1=17.30266295718792\n",
      "SubSGD iter. 228/499: loss=5.429752063966285, w0=73.50000000000014, w1=16.854003918403208\n",
      "SubSGD iter. 229/499: loss=5.508145282566692, w0=74.20000000000014, w1=16.458027675181377\n",
      "SubSGD iter. 230/499: loss=5.8066347681030255, w0=74.90000000000015, w1=13.687727978292388\n",
      "SubSGD iter. 231/499: loss=5.994579594079048, w0=75.60000000000015, w1=13.801548305106701\n",
      "SubSGD iter. 232/499: loss=5.696681598733405, w0=74.90000000000015, w1=14.458429272859664\n",
      "SubSGD iter. 233/499: loss=5.466344998486303, w0=74.20000000000014, w1=15.584361686360836\n",
      "SubSGD iter. 234/499: loss=5.6526434187646695, w0=74.90000000000015, w1=14.906014690386293\n",
      "SubSGD iter. 235/499: loss=5.464865160951369, w0=74.20000000000014, w1=15.714128431428225\n",
      "SubSGD iter. 236/499: loss=5.392900228335003, w0=73.50000000000014, w1=16.522242172470158\n",
      "SubSGD iter. 237/499: loss=5.388092914796901, w0=72.80000000000014, w1=16.945481597242672\n",
      "SubSGD iter. 238/499: loss=5.425755249992246, w0=73.50000000000014, w1=16.818467152014648\n",
      "SubSGD iter. 239/499: loss=5.31760361639724, w0=72.80000000000014, w1=16.290411336504\n",
      "SubSGD iter. 240/499: loss=5.338649210141234, w0=72.10000000000014, w1=16.091650470685476\n",
      "SubSGD iter. 241/499: loss=5.443997386949898, w0=71.40000000000013, w1=16.461918561214272\n",
      "SubSGD iter. 242/499: loss=5.345496934675066, w0=72.10000000000014, w1=15.677685833174948\n",
      "SubSGD iter. 243/499: loss=5.445884164175284, w0=71.40000000000013, w1=15.635359276571457\n",
      "SubSGD iter. 244/499: loss=5.48730654733825, w0=72.10000000000014, w1=14.451357076177565\n",
      "SubSGD iter. 245/499: loss=5.402823875208966, w0=72.80000000000014, w1=14.653793966218565\n",
      "SubSGD iter. 246/499: loss=5.375638753573379, w0=73.50000000000014, w1=15.299941199605787\n",
      "SubSGD iter. 247/499: loss=5.323817802538871, w0=72.80000000000014, w1=15.57987019204225\n",
      "SubSGD iter. 248/499: loss=5.865077651516786, w0=73.50000000000014, w1=12.809570495153261\n",
      "SubSGD iter. 249/499: loss=5.761082207218596, w0=74.20000000000014, w1=13.249494938037444\n",
      "SubSGD iter. 250/499: loss=5.581066952512771, w0=73.50000000000014, w1=13.680505622786058\n",
      "SubSGD iter. 251/499: loss=5.4241747119020545, w0=72.80000000000014, w1=14.499163992591686\n",
      "SubSGD iter. 252/499: loss=5.748775987830458, w0=72.10000000000014, w1=13.442762761212796\n",
      "SubSGD iter. 253/499: loss=5.516617674827115, w0=72.80000000000014, w1=13.995968677288909\n",
      "SubSGD iter. 254/499: loss=5.4022368305784925, w0=73.50000000000014, w1=14.837158845513551\n",
      "SubSGD iter. 255/499: loss=5.319701674402191, w0=72.80000000000014, w1=15.65581721531918\n",
      "SubSGD iter. 256/499: loss=5.355780323698362, w0=72.10000000000014, w1=16.544649039485673\n",
      "SubSGD iter. 257/499: loss=5.493726638452483, w0=71.40000000000013, w1=17.102772743234713\n",
      "SubSGD iter. 258/499: loss=5.6127817703888105, w0=70.70000000000013, w1=16.40183874579996\n",
      "SubSGD iter. 259/499: loss=5.445103914037222, w0=71.40000000000013, w1=16.490366259289086\n",
      "SubSGD iter. 260/499: loss=5.487348892572505, w0=72.10000000000014, w1=17.503044600641854\n",
      "SubSGD iter. 261/499: loss=5.6400308784533735, w0=71.40000000000013, w1=17.782973593078317\n",
      "SubSGD iter. 262/499: loss=5.757693517857062, w0=70.70000000000013, w1=17.532692476616262\n",
      "SubSGD iter. 263/499: loss=5.925347746597868, w0=70.00000000000013, w1=16.87550977342128\n",
      "SubSGD iter. 264/499: loss=6.266742158818256, w0=69.30000000000013, w1=17.160684267970574\n",
      "SubSGD iter. 265/499: loss=6.601044119080197, w0=68.60000000000012, w1=16.899972046588708\n",
      "SubSGD iter. 266/499: loss=6.234233460059172, w0=69.30000000000013, w1=16.88139258768983\n",
      "SubSGD iter. 267/499: loss=5.929297388195318, w0=70.00000000000013, w1=16.9186798993077\n",
      "SubSGD iter. 268/499: loss=5.61623204421151, w0=70.70000000000013, w1=15.828884876234481\n",
      "SubSGD iter. 269/499: loss=5.5281637320225885, w0=71.40000000000013, w1=14.801952678641948\n",
      "SubSGD iter. 270/499: loss=5.658562858682493, w0=70.70000000000013, w1=15.172220769170744\n",
      "SubSGD iter. 271/499: loss=5.595321963662004, w0=71.40000000000013, w1=14.493873773196201\n",
      "SubSGD iter. 272/499: loss=5.637645879868414, w0=70.70000000000013, w1=15.395506037608959\n",
      "SubSGD iter. 273/499: loss=5.496448632987612, w0=71.40000000000013, w1=15.059143079932882\n",
      "SubSGD iter. 274/499: loss=5.512912814386078, w0=72.10000000000014, w1=14.319810280488937\n",
      "SubSGD iter. 275/499: loss=5.324951437663989, w0=72.80000000000014, w1=15.559126581466938\n",
      "SubSGD iter. 276/499: loss=5.341066353330484, w0=72.10000000000014, w1=16.216007549219903\n",
      "SubSGD iter. 277/499: loss=5.46714408730429, w0=71.40000000000013, w1=15.336277419387425\n",
      "SubSGD iter. 278/499: loss=5.344572519213065, w0=72.10000000000014, w1=15.714817202744157\n",
      "SubSGD iter. 279/499: loss=5.496624296622924, w0=71.40000000000013, w1=15.057634499549172\n",
      "SubSGD iter. 280/499: loss=5.628142768470356, w0=70.70000000000013, w1=15.562652231373495\n",
      "SubSGD iter. 281/499: loss=5.434188626635787, w0=71.40000000000013, w1=16.09235320543561\n",
      "SubSGD iter. 282/499: loss=5.62895224484177, w0=70.70000000000013, w1=16.65047690918465\n",
      "SubSGD iter. 283/499: loss=5.915728375351942, w0=70.00000000000013, w1=16.77036872267298\n",
      "SubSGD iter. 284/499: loss=5.621403979657176, w0=70.70000000000013, w1=16.565535202797758\n",
      "SubSGD iter. 285/499: loss=5.441247199916058, w0=71.40000000000013, w1=15.746876832992129\n",
      "SubSGD iter. 286/499: loss=5.348887627233985, w0=72.10000000000014, w1=16.410021150035178\n",
      "SubSGD iter. 287/499: loss=5.44036439311204, w0=71.40000000000013, w1=16.367694593431686\n",
      "SubSGD iter. 288/499: loss=5.6586980159202165, w0=70.70000000000013, w1=16.925818297180726\n",
      "SubSGD iter. 289/499: loss=5.434878142645027, w0=71.40000000000013, w1=16.186485497736783\n",
      "SubSGD iter. 290/499: loss=5.644659133140134, w0=70.70000000000013, w1=16.81130159754773\n",
      "SubSGD iter. 291/499: loss=5.5193175485242065, w0=71.40000000000013, w1=17.25122604043191\n",
      "SubSGD iter. 292/499: loss=5.663469307023213, w0=70.70000000000013, w1=16.962728775874492\n",
      "SubSGD iter. 293/499: loss=5.447211190503812, w0=71.40000000000013, w1=16.532397187413757\n",
      "SubSGD iter. 294/499: loss=5.621578630274299, w0=72.10000000000014, w1=18.072459525800944\n",
      "SubSGD iter. 295/499: loss=5.5180586679574715, w0=72.80000000000014, w1=17.71934377423363\n",
      "SubSGD iter. 296/499: loss=5.5357051351939095, w0=72.10000000000014, w1=17.73309897020912\n",
      "SubSGD iter. 297/499: loss=5.407551549519876, w0=72.80000000000014, w1=17.076218002456155\n",
      "SubSGD iter. 298/499: loss=5.474010731593019, w0=72.10000000000014, w1=17.432943520811992\n",
      "SubSGD iter. 299/499: loss=5.688828045569654, w0=71.40000000000013, w1=17.968463203638198\n",
      "SubSGD iter. 300/499: loss=5.717790409299583, w0=72.10000000000014, w1=18.408387646522378\n",
      "SubSGD iter. 301/499: loss=5.595830063356249, w0=72.80000000000014, w1=18.058449715042062\n",
      "SubSGD iter. 302/499: loss=5.7425655782597325, w0=72.10000000000014, w1=18.490242293325483\n",
      "SubSGD iter. 303/499: loss=5.551709357789672, w0=72.80000000000014, w1=17.8711011379111\n",
      "SubSGD iter. 304/499: loss=5.498072918745732, w0=73.50000000000014, w1=17.267335067944586\n",
      "SubSGD iter. 305/499: loss=5.5683885725112585, w0=74.20000000000014, w1=17.011903186843014\n",
      "SubSGD iter. 306/499: loss=5.397415555250725, w0=73.50000000000014, w1=16.563244148058303\n",
      "SubSGD iter. 307/499: loss=5.320414898097827, w0=72.80000000000014, w1=16.38489202667849\n",
      "SubSGD iter. 308/499: loss=5.346828692260631, w0=72.10000000000014, w1=15.638906373091093\n",
      "SubSGD iter. 309/499: loss=5.434141812845745, w0=71.40000000000013, w1=16.116954575517216\n",
      "SubSGD iter. 310/499: loss=5.339623588041845, w0=72.10000000000014, w1=16.159281132120707\n",
      "SubSGD iter. 311/499: loss=5.326093456749957, w0=72.80000000000014, w1=15.538229558550114\n",
      "SubSGD iter. 312/499: loss=5.344393997801916, w0=72.10000000000014, w1=15.722390336868434\n",
      "SubSGD iter. 313/499: loss=5.438021823157651, w0=71.40000000000013, w1=15.842894888347477\n",
      "SubSGD iter. 314/499: loss=5.407578370507893, w0=72.10000000000014, w1=14.989048617922208\n",
      "SubSGD iter. 315/499: loss=5.312768995724703, w0=72.80000000000014, w1=15.845270670426746\n",
      "SubSGD iter. 316/499: loss=5.343134333203359, w0=72.10000000000014, w1=16.277063248710167\n",
      "SubSGD iter. 317/499: loss=5.33499037377517, w0=72.80000000000014, w1=15.37543098429741\n",
      "SubSGD iter. 318/499: loss=5.3637200968362695, w0=72.10000000000014, w1=15.369003103419063\n",
      "SubSGD iter. 319/499: loss=5.4355432833416994, w0=71.40000000000013, w1=16.21042161314501\n",
      "SubSGD iter. 320/499: loss=5.347291770171194, w0=72.10000000000014, w1=16.376227872712505\n",
      "SubSGD iter. 321/499: loss=5.395108161954798, w0=72.80000000000014, w1=16.992614821549754\n",
      "SubSGD iter. 322/499: loss=5.347229152472601, w0=72.10000000000014, w1=15.627245368886143\n",
      "SubSGD iter. 323/499: loss=5.339610940280812, w0=72.80000000000014, w1=15.290882411210067\n",
      "SubSGD iter. 324/499: loss=5.366231930184014, w0=73.50000000000014, w1=15.535195134919201\n",
      "SubSGD iter. 325/499: loss=5.313228218100784, w0=72.80000000000014, w1=16.043696252659036\n",
      "SubSGD iter. 326/499: loss=5.339939117921172, w0=72.10000000000014, w1=15.911372726305197\n",
      "SubSGD iter. 327/499: loss=5.449812971031171, w0=71.40000000000013, w1=15.556345248775372\n",
      "SubSGD iter. 328/499: loss=5.339938395691476, w0=72.10000000000014, w1=16.17273219761262\n",
      "SubSGD iter. 329/499: loss=5.314390335490155, w0=72.80000000000014, w1=16.140303189845262\n",
      "SubSGD iter. 330/499: loss=5.367536936582766, w0=72.10000000000014, w1=16.710263314381248\n",
      "SubSGD iter. 331/499: loss=5.313047649868343, w0=72.80000000000014, w1=16.02356867887511\n",
      "SubSGD iter. 332/499: loss=5.352311291041588, w0=72.10000000000014, w1=15.52794241840628\n",
      "SubSGD iter. 333/499: loss=5.416693827231477, w0=72.80000000000014, w1=14.551144939464361\n",
      "SubSGD iter. 334/499: loss=5.395345689178503, w0=72.10000000000014, w1=15.07810197523321\n",
      "SubSGD iter. 335/499: loss=5.460759939448803, w0=72.80000000000014, w1=14.28373263074243\n",
      "SubSGD iter. 336/499: loss=5.373713179168252, w0=73.50000000000014, w1=15.341647937569988\n",
      "SubSGD iter. 337/499: loss=5.460759939448803, w0=72.80000000000014, w1=14.28373263074243\n",
      "SubSGD iter. 338/499: loss=5.442136460161414, w0=73.50000000000014, w1=14.427063221336946\n",
      "SubSGD iter. 339/499: loss=5.491120833933286, w0=74.20000000000014, w1=14.89364534730352\n",
      "SubSGD iter. 340/499: loss=6.488352432076341, w0=74.90000000000015, w1=11.520501669713843\n",
      "SubSGD iter. 341/499: loss=6.3749898864189625, w0=75.60000000000015, w1=12.133494166897403\n",
      "SubSGD iter. 342/499: loss=6.519747563414174, w0=76.30000000000015, w1=12.411621135699091\n",
      "SubSGD iter. 343/499: loss=6.582995178684709, w0=77.00000000000016, w1=13.592425836398988\n",
      "SubSGD iter. 344/499: loss=6.303559744221166, w0=76.30000000000015, w1=13.449095245804472\n",
      "SubSGD iter. 345/499: loss=5.98949035122328, w0=75.60000000000015, w1=13.8343793861326\n",
      "SubSGD iter. 346/499: loss=6.0507293686669295, w0=74.90000000000015, w1=12.639640549028174\n",
      "SubSGD iter. 347/499: loss=6.058309606906612, w0=75.60000000000015, w1=13.400997499647243\n",
      "SubSGD iter. 348/499: loss=6.152956093530385, w0=76.30000000000015, w1=14.52612745977755\n",
      "SubSGD iter. 349/499: loss=5.853220220774282, w0=75.60000000000015, w1=15.61592248285077\n",
      "SubSGD iter. 350/499: loss=5.623983138289048, w0=74.90000000000015, w1=15.27220986166738\n",
      "SubSGD iter. 351/499: loss=5.879240826488222, w0=75.60000000000015, w1=16.004174455175015\n",
      "SubSGD iter. 352/499: loss=5.71713373150304, w0=74.90000000000015, w1=16.812288196216947\n",
      "SubSGD iter. 353/499: loss=5.480271567004572, w0=74.20000000000014, w1=16.078012452725147\n",
      "SubSGD iter. 354/499: loss=5.405442759862698, w0=73.50000000000014, w1=16.636136156474187\n",
      "SubSGD iter. 355/499: loss=5.365109699892193, w0=72.80000000000014, w1=16.786592832925315\n",
      "SubSGD iter. 356/499: loss=5.473794167968283, w0=73.50000000000014, w1=17.13106107979578\n",
      "SubSGD iter. 357/499: loss=5.714249304596071, w0=74.20000000000014, w1=17.76333550827747\n",
      "SubSGD iter. 358/499: loss=5.478736776210773, w0=73.50000000000014, w1=17.158803402423313\n",
      "SubSGD iter. 359/499: loss=5.476888355294427, w0=72.80000000000014, w1=17.505887388043305\n",
      "SubSGD iter. 360/499: loss=5.689591827901287, w0=72.10000000000014, w1=18.314001129085238\n",
      "SubSGD iter. 361/499: loss=5.654055966035572, w0=72.80000000000014, w1=18.28157212131788\n",
      "SubSGD iter. 362/499: loss=5.650765054644084, w0=73.50000000000014, w1=18.04059220665123\n",
      "SubSGD iter. 363/499: loss=5.8383386241861945, w0=74.20000000000014, w1=18.284904930360366\n",
      "SubSGD iter. 364/499: loss=6.403974039502474, w0=74.90000000000015, w1=19.41003489049067\n",
      "SubSGD iter. 365/499: loss=6.425088938783691, w0=74.20000000000014, w1=19.96200632242835\n",
      "SubSGD iter. 366/499: loss=6.191535003148838, w0=73.50000000000014, w1=19.711725205966296\n",
      "SubSGD iter. 367/499: loss=6.515671909624308, w0=72.80000000000014, w1=20.54776406323911\n",
      "SubSGD iter. 368/499: loss=6.257362061391121, w0=73.50000000000014, w1=19.869417067264568\n",
      "SubSGD iter. 369/499: loss=5.8251521247437825, w0=72.80000000000014, w1=18.8567387259118\n",
      "SubSGD iter. 370/499: loss=5.703500003693357, w0=72.10000000000014, w1=18.361112465442968\n",
      "SubSGD iter. 371/499: loss=5.836088468257985, w0=71.40000000000013, w1=18.48161701692201\n",
      "SubSGD iter. 372/499: loss=5.7112168132045555, w0=70.70000000000013, w1=17.275600727151563\n",
      "SubSGD iter. 373/499: loss=6.039809865057195, w0=70.00000000000013, w1=17.707393305434984\n",
      "SubSGD iter. 374/499: loss=5.745084898685752, w0=70.70000000000013, w1=17.466413390768334\n",
      "SubSGD iter. 375/499: loss=5.521394700931764, w0=71.40000000000013, w1=17.26157987089311\n",
      "SubSGD iter. 376/499: loss=5.462723456767493, w0=72.10000000000014, w1=17.371879904353904\n",
      "SubSGD iter. 377/499: loss=5.352123412358203, w0=72.80000000000014, w1=16.69353290837936\n",
      "SubSGD iter. 378/499: loss=5.341145522245146, w0=72.10000000000014, w1=15.860195325819785\n",
      "SubSGD iter. 379/499: loss=5.776251898590506, w0=72.80000000000014, w1=13.089895628930796\n",
      "SubSGD iter. 380/499: loss=5.712710349363873, w0=73.50000000000014, w1=13.233226219525312\n",
      "SubSGD iter. 381/499: loss=5.559777501098581, w0=74.20000000000014, w1=14.245664557391732\n",
      "SubSGD iter. 382/499: loss=5.6249503821829006, w0=74.90000000000015, w1=15.2583428987445\n",
      "SubSGD iter. 383/499: loss=5.855516829210105, w0=75.60000000000015, w1=15.685859983961919\n",
      "SubSGD iter. 384/499: loss=5.636234290597486, w0=74.90000000000015, w1=15.971034478511212\n",
      "SubSGD iter. 385/499: loss=5.9554747666817835, w0=75.60000000000015, w1=16.702999072018848\n",
      "SubSGD iter. 386/499: loss=5.63606515756288, w0=74.90000000000015, w1=15.968723328527048\n",
      "SubSGD iter. 387/499: loss=5.849800109305887, w0=75.60000000000015, w1=15.414616254530467\n",
      "SubSGD iter. 388/499: loss=6.2468261708799835, w0=76.30000000000015, w1=16.539746214660774\n",
      "SubSGD iter. 389/499: loss=5.9145301179141825, w0=75.60000000000015, w1=16.36139409328096\n",
      "SubSGD iter. 390/499: loss=5.687320349663702, w0=74.90000000000015, w1=16.566227613156183\n",
      "SubSGD iter. 391/499: loss=5.488470517987854, w0=74.20000000000014, w1=16.211200135626356\n",
      "SubSGD iter. 392/499: loss=5.629072144711245, w0=74.90000000000015, w1=15.835614297339786\n",
      "SubSGD iter. 393/499: loss=5.495988208233712, w0=74.20000000000014, w1=16.31366249976591\n",
      "SubSGD iter. 394/499: loss=5.418431636927553, w0=73.50000000000014, w1=16.753350904851096\n",
      "SubSGD iter. 395/499: loss=5.567156264919419, w0=74.20000000000014, w1=17.002949367538275\n",
      "SubSGD iter. 396/499: loss=5.77819939985964, w0=74.90000000000015, w1=17.21411376210448\n",
      "SubSGD iter. 397/499: loss=6.0698193627728365, w0=75.60000000000015, w1=17.379566554037105\n",
      "SubSGD iter. 398/499: loss=5.952279778866339, w0=74.90000000000015, w1=18.00438265384805\n",
      "SubSGD iter. 399/499: loss=5.861448213565991, w0=74.20000000000014, w1=18.374650744376847\n",
      "SubSGD iter. 400/499: loss=5.544205425385876, w0=73.50000000000014, w1=17.51842869187231\n",
      "SubSGD iter. 401/499: loss=5.469758402802244, w0=72.80000000000014, w1=17.4662683463426\n",
      "SubSGD iter. 402/499: loss=5.38300203374083, w0=72.10000000000014, w1=16.847134950397933\n",
      "SubSGD iter. 403/499: loss=5.434239775774969, w0=71.40000000000013, w1=16.14620095296318\n",
      "SubSGD iter. 404/499: loss=5.622511077346742, w0=70.70000000000013, w1=16.5779935312466\n",
      "SubSGD iter. 405/499: loss=5.464415506563822, w0=71.40000000000013, w1=15.366570643320422\n",
      "SubSGD iter. 406/499: loss=5.34216964927195, w0=72.10000000000014, w1=15.816750389526716\n",
      "SubSGD iter. 407/499: loss=5.313780177196114, w0=72.80000000000014, w1=15.792784549723624\n",
      "SubSGD iter. 408/499: loss=5.338151530698474, w0=72.10000000000014, w1=15.993534086772875\n",
      "SubSGD iter. 409/499: loss=5.370730514154292, w0=72.80000000000014, w1=16.82687166933245\n",
      "SubSGD iter. 410/499: loss=5.369962521982268, w0=73.50000000000014, w1=16.27276459533587\n",
      "SubSGD iter. 411/499: loss=5.34581052663817, w0=72.80000000000014, w1=15.18888919864457\n",
      "SubSGD iter. 412/499: loss=5.385454948970193, w0=73.50000000000014, w1=15.088612099404362\n",
      "SubSGD iter. 413/499: loss=5.450241885170634, w0=72.80000000000014, w1=14.342626445816965\n",
      "SubSGD iter. 414/499: loss=5.57701104519471, w0=72.10000000000014, w1=14.05412918125955\n",
      "SubSGD iter. 415/499: loss=5.486912050686622, w0=71.40000000000013, w1=15.14392420433277\n",
      "SubSGD iter. 416/499: loss=5.359774352584314, w0=72.10000000000014, w1=15.422051173134458\n",
      "SubSGD iter. 417/499: loss=5.437685933233304, w0=71.40000000000013, w1=15.862246427841429\n",
      "SubSGD iter. 418/499: loss=5.613153147708312, w0=70.70000000000013, w1=15.98213824132976\n",
      "SubSGD iter. 419/499: loss=5.434825057071452, w0=71.40000000000013, w1=16.18457513137076\n",
      "SubSGD iter. 420/499: loss=5.644447436958774, w0=70.70000000000013, w1=16.809391231181706\n",
      "SubSGD iter. 421/499: loss=5.973887083660927, w0=70.00000000000013, w1=17.299074764578293\n",
      "SubSGD iter. 422/499: loss=5.78042690510318, w0=70.70000000000013, w1=17.64354301144876\n",
      "SubSGD iter. 423/499: loss=6.056440475977049, w0=70.00000000000013, w1=17.793999687899888\n",
      "SubSGD iter. 424/499: loss=6.220140344918242, w0=69.30000000000013, w1=16.73608438107233\n",
      "SubSGD iter. 425/499: loss=6.616731835359407, w0=68.60000000000012, w1=17.083168366692323\n",
      "SubSGD iter. 426/499: loss=6.191342016821948, w0=69.30000000000013, w1=16.139661883155203\n",
      "SubSGD iter. 427/499: loss=5.904610400381579, w0=70.00000000000013, w1=16.648847727101455\n",
      "SubSGD iter. 428/499: loss=5.667927886576296, w0=70.70000000000013, w1=16.99331597397192\n",
      "SubSGD iter. 429/499: loss=5.553778294699394, w0=71.40000000000013, w1=17.420833059189338\n",
      "SubSGD iter. 430/499: loss=5.352248501672089, w0=72.10000000000014, w1=16.477326575652217\n",
      "SubSGD iter. 431/499: loss=5.312785270611585, w0=72.80000000000014, w1=15.856275002081624\n",
      "SubSGD iter. 432/499: loss=5.360219229784585, w0=73.50000000000014, w1=15.99960559267614\n",
      "SubSGD iter. 433/499: loss=5.46551987435438, w0=74.20000000000014, w1=15.758625678009492\n",
      "SubSGD iter. 434/499: loss=5.620436048896131, w0=74.90000000000015, w1=15.328294089548756\n",
      "SubSGD iter. 435/499: loss=5.864014552472225, w0=75.60000000000015, w1=14.897962501088019\n",
      "SubSGD iter. 436/499: loss=6.168806957594294, w0=76.30000000000015, w1=14.37332957637628\n",
      "SubSGD iter. 437/499: loss=6.499725499300484, w0=77.00000000000016, w1=15.61264587735428\n",
      "SubSGD iter. 438/499: loss=6.17803563585985, w0=76.30000000000015, w1=16.008082669297288\n",
      "SubSGD iter. 439/499: loss=5.850281323555518, w0=75.60000000000015, w1=15.478381695235171\n",
      "SubSGD iter. 440/499: loss=6.150155072417879, w0=76.30000000000015, w1=15.493689361947308\n",
      "SubSGD iter. 441/499: loss=6.463144091058586, w0=77.00000000000016, w1=14.920679278932685\n",
      "SubSGD iter. 442/499: loss=6.135267368433827, w0=76.30000000000015, w1=14.934434474908175\n",
      "SubSGD iter. 443/499: loss=5.925131007059623, w0=75.60000000000015, w1=14.315301078963508\n",
      "SubSGD iter. 444/499: loss=5.645320092764657, w0=74.90000000000015, w1=14.99364807493805\n",
      "SubSGD iter. 445/499: loss=5.853088018140855, w0=75.60000000000015, w1=15.6100350237753\n",
      "SubSGD iter. 446/499: loss=5.702401014420084, w0=74.90000000000015, w1=16.69983004684852\n",
      "SubSGD iter. 447/499: loss=5.512403704083917, w0=74.20000000000014, w1=16.501069181029994\n",
      "SubSGD iter. 448/499: loss=5.511462207665023, w0=73.50000000000014, w1=17.342487690755938\n",
      "SubSGD iter. 449/499: loss=5.720869156904713, w0=74.20000000000014, w1=17.792667436962233\n",
      "SubSGD iter. 450/499: loss=5.711232763229704, w0=73.50000000000014, w1=18.301168554702066\n",
      "SubSGD iter. 451/499: loss=5.580390427547582, w0=74.20000000000014, w1=17.089745666775887\n",
      "SubSGD iter. 452/499: loss=5.42381866017852, w0=73.50000000000014, w1=16.80124840221847\n",
      "SubSGD iter. 453/499: loss=5.322361964900787, w0=72.80000000000014, w1=15.606509565114044\n",
      "SubSGD iter. 454/499: loss=5.367760390936618, w0=72.10000000000014, w1=15.318012300556628\n",
      "SubSGD iter. 455/499: loss=5.444604673037204, w0=71.40000000000013, w1=15.663189388281626\n",
      "SubSGD iter. 456/499: loss=5.343227240112098, w0=72.10000000000014, w1=16.279576337118876\n",
      "SubSGD iter. 457/499: loss=5.470839917350311, w0=71.40000000000013, w1=16.902362743939168\n",
      "SubSGD iter. 458/499: loss=5.61574580616754, w0=70.70000000000013, w1=15.845961512560278\n",
      "SubSGD iter. 459/499: loss=5.882396137129373, w0=70.00000000000013, w1=16.404085216309316\n",
      "SubSGD iter. 460/499: loss=5.628833925628252, w0=70.70000000000013, w1=15.550238945884047\n",
      "SubSGD iter. 461/499: loss=5.537668896691212, w0=71.40000000000013, w1=14.750408211821338\n",
      "SubSGD iter. 462/499: loss=5.694201433979157, w0=70.70000000000013, w1=14.967665216001087\n",
      "SubSGD iter. 463/499: loss=5.435447689385813, w0=71.40000000000013, w1=16.206981516979088\n",
      "SubSGD iter. 464/499: loss=5.660641425426518, w0=70.70000000000013, w1=16.941095839980076\n",
      "SubSGD iter. 465/499: loss=5.450157349251976, w0=71.40000000000013, w1=16.590758906611683\n",
      "SubSGD iter. 466/499: loss=5.347749726530349, w0=72.10000000000014, w1=16.38592538673646\n",
      "SubSGD iter. 467/499: loss=5.5105765181745525, w0=71.40000000000013, w1=17.20458375654209\n",
      "SubSGD iter. 468/499: loss=5.612937113936398, w0=70.70000000000013, w1=15.998567466771641\n",
      "SubSGD iter. 469/499: loss=5.87283117966201, w0=70.00000000000013, w1=15.50294120630281\n",
      "SubSGD iter. 470/499: loss=5.611000168247551, w0=70.70000000000013, w1=16.145870905066605\n",
      "SubSGD iter. 471/499: loss=5.442805568671625, w0=71.40000000000013, w1=15.70618249998142\n",
      "SubSGD iter. 472/499: loss=5.611715549417847, w0=70.70000000000013, w1=16.091466640309548\n",
      "SubSGD iter. 473/499: loss=5.465369397644706, w0=71.40000000000013, w1=16.837452293896945\n",
      "SubSGD iter. 474/499: loss=5.338699454150253, w0=72.10000000000014, w1=16.098119494453\n",
      "SubSGD iter. 475/499: loss=5.437540103548767, w0=71.40000000000013, w1=16.28228027277132\n",
      "SubSGD iter. 476/499: loss=5.354418238423975, w0=72.10000000000014, w1=15.498047544731996\n",
      "SubSGD iter. 477/499: loss=5.31288774669928, w0=72.80000000000014, w1=15.925564629949415\n",
      "SubSGD iter. 478/499: loss=5.339118498850372, w0=72.10000000000014, w1=16.13039814982464\n",
      "SubSGD iter. 479/499: loss=5.359554626795543, w0=72.80000000000014, w1=16.74678509866189\n",
      "SubSGD iter. 480/499: loss=5.3559573906321125, w0=72.10000000000014, w1=16.548024232843364\n",
      "SubSGD iter. 481/499: loss=5.46504008646798, w0=71.40000000000013, w1=16.83319872739266\n",
      "SubSGD iter. 482/499: loss=5.358498228802743, w0=72.10000000000014, w1=16.59221881272601\n",
      "SubSGD iter. 483/499: loss=5.442144650844249, w0=71.40000000000013, w1=16.413866691346197\n",
      "SubSGD iter. 484/499: loss=5.344514417985142, w0=72.10000000000014, w1=15.717281940105433\n",
      "SubSGD iter. 485/499: loss=5.31969098278325, w0=72.80000000000014, w1=16.36933883754789\n",
      "SubSGD iter. 486/499: loss=5.39432115798794, w0=73.50000000000014, w1=16.535145097115386\n",
      "SubSGD iter. 487/499: loss=5.407840825844088, w0=72.80000000000014, w1=17.078161558480485\n",
      "SubSGD iter. 488/499: loss=5.446357691820379, w0=73.50000000000014, w1=16.967987441599742\n",
      "SubSGD iter. 489/499: loss=5.312932092320111, w0=72.80000000000014, w1=15.955549103733322\n",
      "SubSGD iter. 490/499: loss=5.344797622769083, w0=72.10000000000014, w1=15.705267987271265\n",
      "SubSGD iter. 491/499: loss=5.5592495931106365, w0=71.40000000000013, w1=14.648866755892374\n",
      "SubSGD iter. 492/499: loss=5.74158413600186, w0=72.10000000000014, w1=13.464864555498483\n",
      "SubSGD iter. 493/499: loss=5.804873352448702, w0=71.40000000000013, w1=13.736344737481062\n",
      "SubSGD iter. 494/499: loss=5.525664821230926, w0=72.10000000000014, w1=14.26052528893278\n",
      "SubSGD iter. 495/499: loss=5.75391926597819, w0=71.40000000000013, w1=13.905497811402954\n",
      "SubSGD iter. 496/499: loss=5.863006066072021, w0=70.70000000000013, w1=14.18804563048918\n",
      "SubSGD iter. 497/499: loss=5.882013579372815, w0=71.40000000000013, w1=13.491460879248416\n",
      "SubSGD iter. 498/499: loss=5.507225291155419, w0=72.10000000000014, w1=14.347682931752955\n",
      "SubSGD iter. 499/499: loss=5.482728946385749, w0=71.40000000000013, w1=15.183721789025771\n",
      "SubSGD: execution time=0.015 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494450b2209b423a88fbdd7c90dc44e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
